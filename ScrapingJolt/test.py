import requests
from bs4 import BeautifulSoup
import csv
import time

headers = {
    "User-Agent": "Mozilla/5.0"
}

# ========================
# Step 1: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒªãƒ³ã‚¯å–å¾—
# ========================
base_url = "https://www.jolnet.com/transfer/page/{}"
start_page = 1
max_pages = 36 # å¿…è¦ã«å¿œã˜ã¦å¢—ã‚„ã™
all_links = []

for page in range(start_page, max_pages + 1):
    url = base_url.format(page) if page > 1 else "https://www.jolnet.com/transfer/"
    print(f"Fetching: {url}")
    
    res = requests.get(url, headers=headers)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")

    # ãŠçŸ¥ã‚‰ã›ãƒªãƒ³ã‚¯æŠ½å‡º
    items = soup.select("div.latest ul li a")
    for a in items:
        href = a.get("href")
        if href and "/transfer/" in href:
            all_links.append(href)

# é‡è¤‡é™¤å»
all_links = list(set(all_links))
print(f"\nğŸ”— å…¨{len(all_links)}ä»¶ã®è©³ç´°ãƒšãƒ¼ã‚¸URLã‚’å–å¾—ã—ã¾ã—ãŸã€‚\n")

# ========================
# Step 2: å„è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆä¿®æ­£æ¸ˆã¿ï¼‰
# ========================
results = []

for idx, url in enumerate(all_links, 1):
    print(f"Scraping ({idx}/{len(all_links)}): {url}")
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")

        row_data = {"URL": url}

        # â˜… å­¦æ ¡åã®æŠ½å‡º
        h2_title = soup.find("h2", class_="title")
        if h2_title:
            full_title = h2_title.get_text(strip=True)
            school_name = full_title.split()[-1]  # æœ€å¾Œã®å˜èªã ã‘å–ã‚Šå‡ºã™
            row_data["å­¦æ ¡å"] = school_name
            row_data["ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«"] = full_title

        # â˜… ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º
        table = soup.find("table", class_="whiteTable")
        if table:
            for row in table.find_all("tr"):
                th = row.find("th")
                td = row.find("td")
                if th and td:
                    key = th.get_text(strip=True)
                    value = td.get_text(strip=True)
                    row_data[key] = value

        results.append(row_data)

    except Exception as e:
        print(f"âŒ Error at {url}: {e}")
    
    time.sleep(0.25)
# ========================
# Step 3: CSVã«ä¿å­˜
# ========================

# æ‰‹å‹•ã§æŒ‡å®šã™ã‚‹åŸºæœ¬ã‚«ãƒ©ãƒ é †
keys = [
    "å­¦æ ¡å",
    "å­¦å¹´",
    "å‡ºé¡˜æœŸé–“",
    "è©¦é¨“æ—¥",
    "è©¦é¨“å®Ÿæ–½æœŸé–“",
    "ç™ºè¡¨æ—¥",
    "å…¥è©¦ç§‘ç›®",
    "é¢æ¥",
    "å‹Ÿé›†äººå“¡",
    "URL",
    "ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«",
    "æ‰€åœ¨åœ°",
    "é€£çµ¡å…ˆ",
    "è©³ç´°",
]

# âœ… æœªçŸ¥ã®ã‚«ãƒ©ãƒ ï¼ˆä¸Šè¨˜ä»¥å¤–ï¼‰ã‚’æœ«å°¾ã«è¿½åŠ 
known_keys = set(keys)
extra_keys = set()

for r in results:
    for k in r.keys():
        if k not in known_keys:
            extra_keys.add(k)

# è¿½åŠ ã™ã‚‹ã‚­ãƒ¼ã‚’ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ã«ã™ã‚‹ï¼ˆä»»æ„ï¼‰
keys += sorted(extra_keys)

# ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãå‡ºã—
with open("jolnet_transfer_data.csv", "w", encoding="utf-8", newline='') as f:
    writer = csv.DictWriter(f, fieldnames=keys)
    writer.writeheader()
    writer.writerows(results)

print("\nâœ… å®Œäº†ï¼jolnet_transfer_data.csv ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")